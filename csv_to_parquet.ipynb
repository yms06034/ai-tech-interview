{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOBmkFMO6YUxxB7CyJZLdS6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yms06034/ai-tech-interview/blob/main/csv_to_parquet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh6sjJPTPNIg"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq"
      ],
      "metadata": {
        "id": "pXgKuiqVPXoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = './email.csv'\n",
        "parquet_file = \"./email.parquet\"\n",
        "chunksize = 500\n",
        "csv_stream = pd.read_csv(csv_file, sep=';', chunksize=chunksize, low_memory=False)\n",
        "chunk=next(iter(csv_stream))"
      ],
      "metadata": {
        "id": "mWOmxSbmPfjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv= pd.read_csv('./email.csv', sep=';')\n",
        "test_csv.columns"
      ],
      "metadata": {
        "id": "Q6ytvDhcR4rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquet_schema_old = pa.Table.from_pandas(df=chunk).schema\n",
        "parquet_schema_new = pa.schema([\n",
        "    ('Login email', pa.string()),\n",
        "    ('Identifier', pa.string()),\n",
        "    ('First name',pa.string()),\n",
        "    ('Last name', pa.string())\n",
        "])\n",
        "parquet_schema_old == parquet_schema_new"
      ],
      "metadata": {
        "id": "VPJHmIZBSeHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file = './email.csv'\n",
        "parquet_file = \"./email.parquet\"\n",
        "chunksize = 100_000\n",
        "\n",
        "csv_stream = pd.read_csv(csv_file, sep=';', chunksize=chunksize, low_memory=False)\n",
        "\n",
        "for i, chunk in enumerate(csv_stream):\n",
        "    print(\"Chunk\", i)\n",
        "    if i == 0:\n",
        "        parquet_schema = pa.Table.from_pandas(df=chunk).schema\n",
        "        parquet_writer = pq.ParquetWriter(parquet_file, parquet_schema, compression='snappy')\n",
        "    table = pa.Table.from_pandas(chunk, schema=parquet_schema)\n",
        "    parquet_writer.write_table(table)\n",
        "\n",
        "parquet_writer.close()"
      ],
      "metadata": {
        "id": "_P7bOp5hTENA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"CSV to Parquet\").getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "    StructField(\"site\", StringType(), True),\n",
        "    StructField(\"requests\", LongType(), True)\n",
        "])\n",
        "\n",
        "df = spark.read \\\n",
        "    .schema(schema) \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"delimiter\", \"\\t\") \\\n",
        "    .csv(\"/user/hduser/wikipedia/pageviews-by-second-tsv\")\n",
        "\n",
        "df.write.parquet(\"/user/hduser/wikipedia/pageviews-by-second-parquet\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "socYQtazVBU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}